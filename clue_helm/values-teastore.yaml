# Example values for running the teastore SUT
imageRegistry: ghcr.io/clue2-sose25
imagePullPolicy: Always
sutConfigFileName: teastore.yaml
# Namespace where the SUT is deployed. If set, additional RBAC rules will be created for this namespace so the deployer can manage experiment resources there.
# If you want to use an existing service account, set `rbac.create` to false and provide the service account name in `clueDeployer.serviceAccountName`.
# If you set `rbac.create` to false, make sure the service account has the necessary permissions to deploy the SUT.
sutNamespace: "teastore"
sutConfig: |
  config:
    # The name of the SUT
    sut: "teastore"
    # The K8s namespace to deploy the SUT. Use the same namespace as in the "sutNamespace" variable.
    namespace: "teastore"
    # The Git repository url for the SUT code
    sut_git_repo: "https://github.com/ISE-TU-Berlin/sustainable_teastore.git"
    # The path to the root folder of the repo after cloning
    sut_path: "teastore"
    # The path to the helm chart folder for deployment
    helm_chart_path: "teastore/examples/helm"
    # Optional url to the Git repository for the helm chart. Leave empty if it is the same repo as the SUT.
    helm_chart_repo: ""
    # The values file to use
    values_yaml_name: "values.yaml"
    # The k8s service name (used for in-cluster communication)
    workload_target: "teastore-webui"
    # The specific path
    application_endpoint_path: "/tools.descartes.teastore.webui"
    # Default resource limits for all services (used if autoscaling is enabled)
    default_resource_limits:
      # CPU resource limit
      cpu: 1000
      # Memory resource limit
      memory: 1024
    # The time to wait before deploying individual workloads. Specified in seconds
    wait_before_workloads: 10
    # The time to wait after deploying individual workloads. Specified in seconds
    wait_after_workloads: 10

  ### HELM CHART REPLACEMENTS ###
  # The replacements for the Helm chart values file.
  # Can be used for any arbirary value. It will replace all instances of that value in the whole values file.
  # The replacement will be applied only when all conditions are meet.
  # The `__EXPERIMENT_TAG__` placeholder will be replaces with the name of the variant. Additionally, it will be used as a tag for the docker images.
  # Required fields:
  #   value: str - the old value to replace
  #   replacement: str - the new value to use
  #   conditions: object - the conditions to apply the replacement. If not conditions are specified, the replacement will always run.
  #     autoscaling: bool - apply the replacement only if the autoscaling is set to true
  #     autoscaling_type: str - apply the replacement for a specific autoscaling (options: cpu, memory, or full). The `autoscaling` condition needs to be set to true.
  helm_replacements:
    - value: "descartesresearch"
      replacement: "registry:5000/clue"
    - value: "nodeSelector: {}"
      replacement: 'nodeSelector: {"scaphandre": "true"}'
    - value: "pullPolicy: IfNotPresent"
      replacement: "pullPolicy: Always"
    - value: 'tag: ""'
      replacement: 'tag: "__EXPERIMENT_TAG__"'
    - value: "enabled: false"
      replacement: "enabled: true"
      conditions:
        autoscaling: true
    - value: "targetCPUUtilizationPercentage: 80"
      replacement: "# targetCPUUtilizationPercentage: 80"
      conditions:
        autoscaling: true
        autoscaling_type: mem
    - value: "# targetMemoryUtilizationPercentage: 80"
      replacement: "targetMemoryUtilizationPercentage: 80"
      conditions:
        autoscaling: true
        autoscaling_type: mem
    - value: "# targetMemoryUtilizationPercentage: 80"
      replacement: "targetMemoryUtilizationPercentage: 80"
      conditions:
        autoscaling: true
        autoscaling_type: full
  
  ### RESOURCE LIMITS ###
  # A list of resource limits for individual services.
  # Can be left empty if a default limit is specified in the main config above.
  # Required fields:
  # service_name: str - the name of the service
  # limit: object - the limits for the individual service
  #   cpu: int - the cpu limit
  #   memory: int - the memory limit
  resource_limits:
    - service_name: "teastore-auth"
      limit:
        cpu: 450
        memory: 1024
    - service_name: "teastore-webui"
      limit:
        cpu: 1000
        memory: 1500
    - service_name: "teastore-recommender"
      limit:
        cpu: 1000
        memory: 1024
    - service_name: "teastore-image"
      limit:
        cpu: 1000
        memory: 1500
    - service_name: "teastore-all"
      limit:
        cpu: 1000
        memory: 2048

  ### WORKLOADS ###
  # A list of workloads for the SUT. Required fields:
  # name: str - a short name of the variant
  # description: str - a short description of the variant
  # timeout_duration: int - the total timeout for the workload generator
  # workload_settings: JSON - settings for the Locust workload generator. See default SUTs' configs for more detail.
  workloads:
    - name: "shaped"
      description: "Workload with custom load shape behavior."
      timeout_duration: 120
      workload_runtime: 64
      workload_settings:
        {"LOADGENERATOR_STAGE_DURATION": 8, "LOADGENERATOR_MAX_DAILY_USERS": 100}
      locust_files:
        - "sut_configs/workloads/teastore/consumerbehavior.py"
        - "sut_configs/workloads/teastore/loadshapes.py"
        - "sut_configs/workloads/teastore/config.py"
    - name: "rampup"
      description: "Workload that ramps up users at a constant rate."
      timeout_duration: 120
      workload_runtime: 20
      workload_settings: {"LOCUST_SPAWN_RATE": 3, "LOCUST_USERS": 100}
      locust_files:
        - "sut_configs/workloads/teastore/locustfile.py"
    - name: "pausing"
      description: "Workload that starts 20 pausing users, no ramp-up for the duration."
      timeout_duration: 120
      workload_runtime: 64
      workload_settings:
        {"LOCUST_SPAWN_RATE": 1, "LOCUST_USERS": 10, "PAUSE_BACKOFF": 120}
      locust_files:
        - "sut_configs/workloads/teastore/pausing_users.py"
    - name: "fixed"
      description: "Workload that ramps up to max users for at most 1000 requests (failed or successful), running for at most the specified duration."
      timeout_duration: 120
      workload_runtime: 64
      workload_settings:
        {"LOCUST_SPAWN_RATE": 1, "LOCUST_USERS": 100, "MAXIMUM_REQUESTS": 200}
      locust_files:
        - "sut_configs/workloads/teastore/fixed_requests.py"

  ### VARIANTS ###
  # A list of variants for the SUT. Required fields:
  # name: str - a short name of the variant
  # description: str - a short description of the variant
  # target_branch: str - the name of the git repository branch for the variant
  # colocated_workload: bool - if the workload should be deployed in the cluster or on host machine
  # critical_services: list[str] - a list of the names of the critical services to wait for before running the workload
  # autoscaling: enum[str] - the type of autoscaling (options: cpu, memory, full)
  variants:
    - name: "baseline"
      description: "Baseline experiment for the teastore application"
      target_branch: "vanilla"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "runtime-replacement"
      description: "Replaces the JVM runtime with a more recent version."
      target_branch: "runtime-replacement"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "monolith"
      description: "Runs the teastore application as a monolith, without microservices. Not tested with CLUE2."
      target_branch: "monolith"
      colocated_workload: true
      critical_services: ["teastore-all"]
      autoscaling: "cpu"
    - name: "serverless"
      description: "Runs the authentication service in a serverless fashion using Knative. Not tested with CLUE2."
      target_branch: "serverless-auth"
      colocated_workload: true
      critical_services: ["teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "norec"
      description: "Runs the application without the recommender service. Not tested with CLUE2."
      target_branch: "service-reduction"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "lessrec"
      description: "Reduces the number of recommendations shown to users. Not tested with CLUE2."
      target_branch: "feature/lessrecs"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "obs"
      description: "Uses object storage for storing images. Not tested with CLUE2. Not tested with CLUE2."
      target_branch: "feature/object-storage"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "dbopt"
      description: "Implements database optimizations. Not tested with CLUE2."
      target_branch: "feature/db-optimization"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "car"
      description: "Implements carbon-aware retraining for the recommender. Not tested with CLUE2."
      target_branch: "Carbon-Aware-Retraining"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
    - name: "sig"
      description: "Implements static site generation and API gateway. Not tested with CLUE2."
      target_branch: "ssg+api-gateway"
      colocated_workload: true
      critical_services: ["teastore-auth", "teastore-registry", "teastore-webui"]
      autoscaling: "cpu"
clueConfig: |
  config:
    experiment_timeout: 150
    prometheus_url: "http://130.149.158.32:32426"
    docker_registry_address: "ghcr.io/clue2-sose25"
    local_public_ip: "registry"
    local_port: 5000
    remote_platform_arch: "linux/amd64"
    local_platform_arch: "linux/amd64"
    target_utilization: 70
clueDeployer:
  enabled: true
  image: clue2-deployer
  tag: latest
  serviceAccountName: st-cp25clue2-user
  service:
    port: 8000
    type: ClusterIP
    nodePort: 30501
    resultsPort: 8050
  probes:
    path: /api/health
  env:
    DEPLOY_ONLY: "false"
    N_ITERATIONS: "1"
    SUT: teastore
    VARIANTS: baseline
    WORKLOADS: shaped
    PRECONFIGURE_CLUSTER: "false"
    DEPLOY_AS_SERVICE: "true"
    ENABLE_DEBUG: "false"
    HELM_DRIVER: configmap
    SETUP_GRAFANA_DASHBOARD: "true"
    GRAFANA_USERNAME: admin
    GRAFANA_PASSWORD: SECRET
    GRAFANA_URL: http://10.111.147.11
    GRAFANA_PORT: 80
    PROMETHEUS_URL: http://130.149.158.32:32426
  job:
    enabled: false
clueWebui:
  enabled: true
  image: clue2-webui
  tag: latest
  service:
    port: 80
    type: NodePort
    nodePort: 30501
  env:
    API_BASE_URL: http://clue-deployer:8000
    NGINX_RESOLVER: 10.96.0.10
    NGINX_RESOLVER_VALID: 10s
loadGenerator:
  jobEnabled: false
  image: clue2-loadgenerator-builder
  tag: latest
  # Set this to mount a folder path within the chart to include all locust files
  workloadDir: default_workloads_teastore
ingress:
  enabled: false
  host: clue.example.com
  tls: false
rbac:
  create: false